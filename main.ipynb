{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seara/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/seara/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Россия': 0, 'Экономика': 1, 'Силовые структуры': 2, 'Бывший СССР': 3, 'Спорт': 4, 'Забота о себе': 5, 'Строительство': 6, 'Путешествия': 7, 'Наука и техника': 8}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pandarallel import pandarallel\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim import Adam\n",
    "from pandarallel import pandarallel\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_label2id_id2label(class_labels):\n",
    "    id2label = dict(enumerate(class_labels))\n",
    "    label2id = {i: label for label, i in id2label.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "def push_to_hub(model, tokenizer, name):\n",
    "    model.push_to_hub(name)\n",
    "    tokenizer.push_to_hub(name)\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "PANDARALLEL_WORKERS = 8\n",
    "labels = [\n",
    "    \"Россия\",\n",
    "    \"Экономика\",\n",
    "    \"Силовые структуры\",\n",
    "    \"Бывший СССР\",\n",
    "    \"Спорт\",\n",
    "    \"Забота о себе\",\n",
    "    \"Строительство\",\n",
    "    \"Путешествия\",\n",
    "    \"Наука и техника\",\n",
    "]\n",
    "columns = [\"text\", \"label\"]\n",
    "news_path = \"data/news/lenta-ru-news.csv.bz2\"\n",
    "\n",
    "tokenizer = model = \"cointegrated/rubert-tiny2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "label2id, id2label = get_label2id_id2label(labels)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model,\n",
    "    num_labels=len(labels),\n",
    "    problem_type=\"single_label_classification\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "\n",
    "max_length = 256\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "num_epochs = 3\n",
    "\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[(df[\"topic\"] == \"Среда обитания\") & (df[\"tags\"] == \"Дом\")].sample(5)\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\t\", \"\")\n",
    "    text = text.replace(\"\\\\xa0\", \" \")\n",
    "    text = text.split()\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_fontanka():\n",
    "    datasets = []\n",
    "    for year in [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]:\n",
    "        datasets.append(\n",
    "            pd.read_csv(f\"data/news/Fontanka/metatable_{year}.csv\", sep=\"\\t\")\n",
    "        )\n",
    "\n",
    "    def format_labels(text):\n",
    "        # if text in [\"Финансы\", \"Бизнес\"]:\n",
    "        #     return \"Экономика\"\n",
    "        # if text == \"Технологии\":\n",
    "        #     return \"Наука и техника\"\n",
    "        if text == \"Туризм\":\n",
    "            return \"Путешествия\"\n",
    "        if text == \"Общество\":\n",
    "            return \"Россия\"\n",
    "        return text\n",
    "\n",
    "    def read_text(text_id):\n",
    "        year = str(text_id)[:4]\n",
    "        return open(f\"data/news/Fontanka/texts/{year}/fontanka_{text_id}.txt\").read()\n",
    "\n",
    "    fontanka = pd.concat(datasets)\n",
    "    fontanka = fontanka[\n",
    "        fontanka[\"textrubric\"].isin(\n",
    "            [\n",
    "                \"Общество\",\n",
    "                # \"Финансы\",\n",
    "                # \"Проишествия\",\n",
    "                \"Спорт\",\n",
    "                \"Строительство\",\n",
    "                # \"Бизнес\",\n",
    "                \"Туризм\",\n",
    "                # \"Технологии\",\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "    fontanka = fontanka[[\"textid\", \"textrubric\"]]\n",
    "    fontanka[\"textrubric\"] = fontanka[\"textrubric\"].apply(format_labels)\n",
    "    fontanka[\"textid\"] = fontanka[\"textid\"].astype(int)\n",
    "    fontanka[\"textid\"] = fontanka[\"textid\"].apply(read_text)\n",
    "    fontanka.columns = [\"text\", \"label\"]\n",
    "    fontanka[\"label\"] = fontanka[\"label\"].apply(lambda x: label2id[x])\n",
    "    fontanka[\"text\"] = fontanka[\"text\"].apply(clean_text)\n",
    "    fontanka = fontanka.dropna(subset=\"text\")\n",
    "    return fontanka\n",
    "\n",
    "\n",
    "dataset_path = Path(\"data\", \"news\", \"news.csv\")\n",
    "# if dataset_path.is_file():\n",
    "#     final = pd.read_csv(dataset_path)\n",
    "# else:\n",
    "topics = [\n",
    "    \"Россия\",\n",
    "    \"Экономика\",\n",
    "    \"Силовые структуры\",\n",
    "    \"Бывший СССР\",\n",
    "    \"Спорт\",\n",
    "    \"Забота о себе\",\n",
    "    # \"Среда обитания\",\n",
    "    \"Путешествия\",\n",
    "    \"Наука и техника\",\n",
    "    # \"Дом\",\n",
    "]\n",
    "df = pd.read_csv(news_path)\n",
    "df = df.dropna(subset=\"text\")\n",
    "df = df[df[\"topic\"].isin(topics)][[\"text\", \"topic\"]]\n",
    "# df[\"topic\"] = df[\"topic\"].apply(\n",
    "#     lambda x: \"Строительство\" if x in [\"Среда обитания\", \"Дом\"] else x\n",
    "# )\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "df.columns = [\"text\", \"label\"]\n",
    "df[\"label\"] = df[\"label\"].apply(lambda x: label2id[x])\n",
    "\n",
    "# dataset_2 = pd.read_csv(\"data/news/FULL_data_tokenized_v3.csv\")\n",
    "# dataset_2 = dataset_2.drop(\"processed\", axis=1)\n",
    "# dataset_2.columns = [\"text\", \"label\"]\n",
    "# dataset_2[\"text\"] = dataset_2[\"text\"].apply(clean_text)\n",
    "\n",
    "fontanka = get_fontanka()\n",
    "\n",
    "final = pd.concat([df, fontanka])\n",
    "# final.to_csv(dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash_words = [\"Фото:\", \"Кадр:\", \"PAN Photo\", \"Изображение:\", \"Поделиться\"]\n",
    "sources = [\n",
    "    \"Globallookpress.com\",\n",
    "    \"РИА Новости\",\n",
    "    \"«Коммерсантъ»\",\n",
    "    \"Коммерсант\",\n",
    "    \"Коммерсантъ\",\n",
    "    \"Reuter\",\n",
    "    \"Reuters\",\n",
    "    \"Unsplash\",\n",
    "    \"Getty Images\",\n",
    "    \"Global Look Press\",\n",
    "    \"Depositphotos\",\n",
    "    \"Агентство «Москва»\",\n",
    "    \"ТАСС\",\n",
    "    \"Wikipedia\",\n",
    "    \"«Фонтанка.ру»Поделиться\",\n",
    "    \"«Вестас Рус»\",\n",
    "    \"Depositphotos\",\n",
    "    \"Unsplash.com\",\n",
    "    \"РИА Новости\",\n",
    "    \"YouTubeПоделиться\",\n",
    "    \"Shutterstock\",\n",
    "    \"president.gov.by\",\n",
    "    \"Pixabay\",\n",
    "    \"YouTube\",\n",
    "    \"pixabay.com\",\n",
    "    \"«Лента.ру»\",\n",
    "    \"life.ru\",\n",
    "    \"Boards.4channel.org\",\n",
    "    \"страница Lifedd.ru |\",\n",
    "    \"E1.RUПоделиться\",\n",
    "    \"JAXA\",\n",
    "    \"news.ru via globallookpress.co\",\n",
    "    \"Wikimedia Commons\",\n",
    "    \"Fotodom\",\n",
    "    \"Bloomberg via Getty Images\",\n",
    "    \"Ukran-orosz konfliktus\",\n",
    "    \"Telegram\",\n",
    "    \"«Фонтанка.ру»\",\n",
    "    \"-канал SHOT\",\n",
    "]\n",
    "trash_words.sort(key=len, reverse=True)\n",
    "sources.sort(key=len, reverse=True)\n",
    "\n",
    "\n",
    "def remove_slash_prefix(text: str):\n",
    "    text = text.split()\n",
    "    start = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] in [\"/\", \"\\\\\"] and i <= 6:\n",
    "            start = i\n",
    "    if start != 0:\n",
    "        start += 1\n",
    "    return \" \".join(text[start:])\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(text: str):\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def remove_trash_words(text: str):\n",
    "    for word in trash_words:\n",
    "        text = text.removeprefix(word)\n",
    "        text = text.removeprefix(word.lower())\n",
    "        text = text.removeprefix(word.upper())\n",
    "\n",
    "    for word in sources:\n",
    "        text = text.removeprefix(word)\n",
    "        text = text.removeprefix(word.lower())\n",
    "        text = text.removeprefix(word.upper())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \" \")\n",
    "        .replace(\"\\t\", \"\")\n",
    "        .replace(\"/\", \" / \")\n",
    "        .replace(\"\\\\\", \" \\\\ \")\n",
    "    )\n",
    "    text = remove_extra_whitespaces(text)\n",
    "\n",
    "    text = remove_trash_words(text)\n",
    "\n",
    "    text = remove_slash_prefix(text)\n",
    "    text = remove_slash_prefix(text)\n",
    "\n",
    "    text = remove_trash_words(text)\n",
    "    text = remove_trash_words(text)\n",
    "\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "test = pd.read_csv(\"test_news.csv\")\n",
    "test[\"content\"] = test[\"content\"].apply(preprocess_text)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0918785c98c741be806f01472cb6093c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/786966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(final, preserve_index=False)\n",
    "\n",
    "train_processed = train_dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=max_length),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_processed,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_pandas(test, preserve_index=False)\n",
    "\n",
    "test_processed = test_dataset.map(\n",
    "    lambda x: tokenizer(x['content'], truncation=True, max_length=max_length),\n",
    "    batched=True,\n",
    "    remove_columns=['content'],\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_processed,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79d54667b724d42a61e46514d493e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8397ebb1304ae4bd7aeba7210f4145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013102412223815918\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "optimizer = Adam(params=model.parameters(), lr=0.00002, weight_decay=0)\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(model.device)\n",
    "        output = model(**batch)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    tqdm.write(f\"====================={loss.item()}=====================\")\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            batch = batch.to(model.device)\n",
    "            output = model(**batch)\n",
    "            y_pred.append(output.logits.cpu())\n",
    "\n",
    "    preds = torch.softmax(torch.cat(y_pred), dim=-1)\n",
    "\n",
    "    sub = test.reset_index()\n",
    "    sub[\"topic\"] = torch.argmax(preds, dim=-1).numpy()\n",
    "    sub[[\"topic\", \"index\"]].to_csv(f\"bs-{batch_size}-ep-{num_epochs}-clean-topics.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fb1c86530a4613a992ec06b9bdf03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# push_to_hub(model, tokenizer, \"seara/merged-1-epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb3848306b449f7929f1846a76d8041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96af897211f94556a1104ad5073eb3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trash_words = [\"Фото:\", \"Кадр:\", \"PAN Photo\", \"Изображение:\", \"Поделиться\"]\n",
    "sources = [\n",
    "    \"Globallookpress.com\",\n",
    "    \"РИА Новости\",\n",
    "    \"«Коммерсантъ»\",\n",
    "    \"Коммерсант\",\n",
    "    \"Коммерсантъ\",\n",
    "    \"Reuter\",\n",
    "    \"Reuters\",\n",
    "    \"Unsplash\",\n",
    "    \"Getty Images\",\n",
    "    \"Global Look Press\",\n",
    "    \"Depositphotos\",\n",
    "    \"Агентство «Москва»\",\n",
    "    \"ТАСС\",\n",
    "    \"Wikipedia\",\n",
    "    \"«Фонтанка.ру»Поделиться\",\n",
    "    \"«Вестас Рус»\",\n",
    "    \"Depositphotos\",\n",
    "    \"Unsplash.com\",\n",
    "    \"РИА Новости\",\n",
    "    \"YouTubeПоделиться\",\n",
    "    \"Shutterstock\",\n",
    "    \"president.gov.by\",\n",
    "    \"Pixabay\",\n",
    "    \"YouTube\",\n",
    "    \"pixabay.com\",\n",
    "    \"«Лента.ру»\",\n",
    "    \"life.ru\",\n",
    "    \"Boards.4channel.org\",\n",
    "    \"страница Lifedd.ru |\",\n",
    "    \"E1.RUПоделиться\",\n",
    "    \"JAXA\",\n",
    "    \"news.ru via globallookpress.co\",\n",
    "    \"Wikimedia Commons\",\n",
    "    \"Fotodom\",\n",
    "    \"Bloomberg via Getty Images\",\n",
    "    \"Ukran-orosz konfliktus\",\n",
    "    \"Telegram\",\n",
    "    \"«Фонтанка.ру»\",\n",
    "    \"-канал SHOT\",\n",
    "]\n",
    "trash_words.sort(key=len, reverse=True)\n",
    "sources.sort(key=len, reverse=True)\n",
    "\n",
    "\n",
    "def remove_slash_prefix(text: str):\n",
    "    text = text.split()\n",
    "    start = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] in [\"/\", \"\\\\\"] and i <= 6:\n",
    "            start = i\n",
    "    if start != 0:\n",
    "        start += 1\n",
    "    return \" \".join(text[start:])\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(text: str):\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def remove_trash_words(text: str):\n",
    "    for word in trash_words:\n",
    "        text = text.removeprefix(word)\n",
    "        text = text.removeprefix(word.lower())\n",
    "        text = text.removeprefix(word.upper())\n",
    "\n",
    "    for word in sources:\n",
    "        text = text.removeprefix(word)\n",
    "        text = text.removeprefix(word.lower())\n",
    "        text = text.removeprefix(word.upper())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    text = (\n",
    "        text.replace(\"\\n\", \" \")\n",
    "        .replace(\"\\t\", \"\")\n",
    "        .replace(\"/\", \" / \")\n",
    "        .replace(\"\\\\\", \" \\\\ \")\n",
    "    )\n",
    "    text = remove_extra_whitespaces(text)\n",
    "\n",
    "    text = remove_trash_words(text)\n",
    "\n",
    "    text = remove_slash_prefix(text)\n",
    "    text = remove_slash_prefix(text)\n",
    "\n",
    "    text = remove_trash_words(text)\n",
    "    text = remove_trash_words(text)\n",
    "\n",
    "    text = remove_extra_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "test = pd.read_csv(\"test_news.csv\")\n",
    "test[\"content\"] = test[\"content\"].apply(preprocess_text)\n",
    "test\n",
    "\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test, preserve_index=False)\n",
    "\n",
    "test_processed = test_dataset.map(\n",
    "    lambda x: tokenizer(x['content'], truncation=True, max_length=max_length),\n",
    "    batched=True,\n",
    "    remove_columns=['content'],\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_processed,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        batch = batch.to(model.device)\n",
    "        output = model(**batch)\n",
    "        y_pred.append(output.logits.cpu())\n",
    "\n",
    "preds = torch.softmax(torch.cat(y_pred), dim=-1)\n",
    "\n",
    "sub = test.reset_index()\n",
    "sub[\"topic\"] = torch.argmax(preds, dim=-1).numpy()\n",
    "sub[[\"topic\", \"index\"]].to_csv(f\"bs-{batch_size}-ep-{num_epochs}-clean-topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Экс-министру обороны ДНР Игорю Стрелкову (Гирк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>В начале февраля 2023 года в Пушкинском районе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Анастасия Борисова Международная федерация спо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Если вы хотели, но так и не съездили на море л...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Российский полузащитник «Чертаново» Сергей Пин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26270</th>\n",
       "      <td>Алевтина Запольская Главное управление разведк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26271</th>\n",
       "      <td>Wikimedia Алевтина Запольская Министр молодежи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26272</th>\n",
       "      <td>Александр Курбатов Октябрьский районный суд го...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26273</th>\n",
       "      <td>Варвара Кошечкина Президент Украины Владимир З...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26274</th>\n",
       "      <td>Алевтина Запольская Азербайджану не нужна нова...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26275 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "0      Экс-министру обороны ДНР Игорю Стрелкову (Гирк...\n",
       "1      В начале февраля 2023 года в Пушкинском районе...\n",
       "2      Анастасия Борисова Международная федерация спо...\n",
       "3      Если вы хотели, но так и не съездили на море л...\n",
       "4      Российский полузащитник «Чертаново» Сергей Пин...\n",
       "...                                                  ...\n",
       "26270  Алевтина Запольская Главное управление разведк...\n",
       "26271  Wikimedia Алевтина Запольская Министр молодежи...\n",
       "26272  Александр Курбатов Октябрьский районный суд го...\n",
       "26273  Варвара Кошечкина Президент Украины Владимир З...\n",
       "26274  Алевтина Запольская Азербайджану не нужна нова...\n",
       "\n",
       "[26275 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cecc09ff1f4d63a68437f5762148dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456b2e09eb3d4296b4dae82d79243676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/822 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a342f6427944491834ce26743c1201c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/117M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "push_to_hub(model, tokenizer, \"clean-topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seara/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/pydantic/_internal/_fields.py:128: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/seara/.pyenv/versions/3.9.16/envs/DS/lib/python3.9/site-packages/pydantic/_internal/_config.py:317: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(model=\"seara/merged-1-epoch\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import get_label2id_id2label\n",
    "\n",
    "topics = [\n",
    "    \"Россия\",\n",
    "    \"Экономика\",\n",
    "    \"Силовые структуры\",\n",
    "    \"Бывший СССР\",\n",
    "    \"Спорт\",\n",
    "    \"Забота о себе\",\n",
    "    \"Строительство\",\n",
    "    \"Путешествия\",\n",
    "    \"Наука и техника\",\n",
    "]\n",
    "a, _ = get_label2id_id2label(topics)\n",
    "\n",
    "\n",
    "ans = model(\n",
    "    test[\"content\"].values.tolist(), batch_size=64, truncation=True, max_length=256\n",
    ")\n",
    "out = []\n",
    "for item in ans:\n",
    "    out.append(a[item[\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test.reset_index()\n",
    "sub[\"topic\"] = out\n",
    "sub[[\"topic\", \"index\"]].to_csv(\"submission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Фото: «Фонтанка.ру»ПоделитьсяЭкс-министру обор...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>В начале февраля 2023 года в Пушкинском районе...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Фото: Andy Bao / Getty Images Анастасия Борисо...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Если вы хотели, но так и не съездили на море л...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Сергей Пиняев Фото: Алексей Филиппов / РИА Нов...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26270</th>\n",
       "      <td>26270</td>\n",
       "      <td>Фото: РИА Новости Алевтина Запольская Главное ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26271</th>\n",
       "      <td>26271</td>\n",
       "      <td>Вадим Гутцайт Фото: Sergei CHUZAVKOV / Europea...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26272</th>\n",
       "      <td>26272</td>\n",
       "      <td>Фото: Олег Харсеев / Коммерсантъ Александр Кур...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26273</th>\n",
       "      <td>26273</td>\n",
       "      <td>Владимир Зеленский Фото: Yves Herman / Reuters...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26274</th>\n",
       "      <td>26274</td>\n",
       "      <td>Фото: President of the Republic of Azerbaijan ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                            content  topic\n",
       "0          0  Фото: «Фонтанка.ру»ПоделитьсяЭкс-министру обор...      0\n",
       "1          1  В начале февраля 2023 года в Пушкинском районе...      6\n",
       "2          2  Фото: Andy Bao / Getty Images Анастасия Борисо...      4\n",
       "3          3  Если вы хотели, но так и не съездили на море л...      0\n",
       "4          4  Сергей Пиняев Фото: Алексей Филиппов / РИА Нов...      4\n",
       "...      ...                                                ...    ...\n",
       "26270  26270  Фото: РИА Новости Алевтина Запольская Главное ...      3\n",
       "26271  26271  Вадим Гутцайт Фото: Sergei CHUZAVKOV / Europea...      3\n",
       "26272  26272  Фото: Олег Харсеев / Коммерсантъ Александр Кур...      0\n",
       "26273  26273  Владимир Зеленский Фото: Yves Herman / Reuters...      3\n",
       "26274  26274  Фото: President of the Republic of Azerbaijan ...      3\n",
       "\n",
       "[26275 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>Дарья Рыбак Россиянка отдохнула в «райском уголке» Краснодарского края под Геленджиком и осталась разочарована поездкой. Отзывом о своем путешествии она поделилась в своем блоге на платформе «Яндекс.Дзен». Туристка уточнила, что всегда считала поселок Голубая бухта, который находится в 10 километрах от Геленджика, «райским уголком». По ее воспоминаниям, место привлекало покоем и тишиной, а также славилось голубой водой и дельфинами, «которые подплывали к пирсу». Тем не менее, последняя поездка в это место разочаровала п его, женщина заметила, что «море не такое голубое как раньше» — цвет воды показался ей «мутным и серым». Из плюсов автор назвала недорогую стоимость жилья — так, большой номер в 15 минутах ходьбы от пляжа обходился ей в 1,2 тысячи рублей в сутки. Ранее в июле эта же путешественница бюджетно отдохнула в поселке Ольгинка в Краснодарском крае и также была разочарована чистотой и инфраструктурой пляжа. По словам автора, приехать на курорт ей посоветовали знакомые, которые были в этом месте несколько лет назад, однако место «полностью разочаровало» россиянку.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>Мария Гейн Россиянка застряла в Таиланде почти на месяц и рассказала о попытках вернуться домой. Своим опытом она поделилась с порталом «Тонкости туризма». По словам девушки, как только начала поступать информация о массовой отмене рейсов, она решила переждать ограничения. «Первые несколько дней была растерянность, новости были одна страшнее другой», — призналась она. Также путешествен автор успела на один из последних рейсов Air Astana и добралась до России с пересадкой в Казахстане. Ранее в марте сообщалось, что жители Таиланда начали помогать застрявшим в стране российским туристам. Так, они организовали центры помощи и горячие линии для семи тысяч путешественников из России. Отдыхающим, у которых закончился период безвизового пребывания, продлили возможность нахождения в королевстве на 30 суток.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>Россия и Турция договорились увеличить число авиаперевозок в летнем сезоне. Российские компании смогут выполнять более 720 рейсов по 62 маршрутам, турецкие — 629 рейсов по 37 маршрутам, сообщает «Интерфакс» со ссылкой на Минтранс.Российские авиационные власти уже подтвердили заявки турецких авиакомпаний на дополнительные полеты с суммарной частотой 49 рейсов в неделю. Так, число еженедельно выполняемых рейсов между странами превысит 1,3 тысячи, а это в 1,5 раза больше по сравнению с предыдущим летним сезоном. Согласно договоренностям, еженедельно по маршруту Анталья — Саратов будет совершаться семь рейсов, по маршруту Стамбул — Москва — Анталья — Москва — по 21 ре а кататься по морю на доске, но внезапно усилился ветер, который стал уносить людей от берега. Выбраться отдыхающим помогли другие туристы, которые катались в том же месте на катамаране, — они подтолкнули доску к побережью, где волны были более спокойными. «Женщина с ребенком поплыла, а ветер сильный. (...) И их стало уносить в море, она испугалась, стала орать. Мы, конечно же, пришли на помощь», — рассказала автор видео, отметив, что операция прошла благополучно. Ранее в сети появилось видео, на котором отдыхающие пытались «всем пляжем» спасти женщину, которой стало плохо в море у берега в Сочи. Туристка скончалась на побережье российского курорта до прибытия врачей.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>Лука Сафронов - Затравкин Фото: @lukasafronovofficial Российский пианист, сын художника Никаса Сафронова, Лука Затравкин раскрыл правду о появившейся в СМИ информации по поводу его заключения в турецком отеле из-за поломанного бассейна. Его слова приводит «Пятый канал». Как рассказал Затравкин, сотрудники гостиницы действительно попросили его компенсировать ремонт бассейна, дно которого было пробито в результате неудачного прыжка музыканта. Тем не менее, по его заверениям, никто не удерживает его в п за ремонт сломанного им бассейна. Отец пианиста сообщил, что музыкант не может вернуться в Россию из-за того, что сотрудники гостиницы заставляют его компенсировать ремонт конструкции — турки оценили работы в 1,4 миллиона рублей. 7 сентября Затравкин получил травму во время отпуска в Турции, прыгнув в детский бассейн. По словам артиста, когда он отдыхал в отеле города Белек и хотел прыгнуть в воду, то перепутал детский бассейн со взрослым. В результате он сильно ударился о дно коленями и поломал дно.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>Мария Гейн Россия снова стала лидером по турпотоку в Турцию — за первые девять месяцев 2021 года страну посетили 3,46 миллиона россиян. Об этом сообщается на сайте Ассоциации туроператоров России (АТОР). По данным министерства культуры и туризма республики, с января по сентябрь турпоток в страну из-за рубежа составил 17,5 миллиона человек. Это на 86 процентов больше, чем за аналогичный рибыло 1,7 миллиона человек. Замыкают пятерку лидеров Болгария и Иран. Ранее в сентябре сообщалось, что с января по сентябрь 2021 года Сейшельские острова посетили 19,6 тысячи россиян — это на 133 процента больше, чем за аналогичный период в 2019-м. По данным Национального бюро статистики республики, отечественные туристы обогнали иностранных путешественников по объему турпотока в страну, заняв первое место на въездном рынке.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19762</th>\n",
       "      <td>19762</td>\n",
       "      <td>Александра Качан Карты российской платежной системы «Мир» начали работать на значительной части Венесуэлы. Об этом сообщил директор латиноамериканского департамента Министерства иностранных дел (МИД) России Александр Щетинин, передает ТАСС. «Карты \"Мир\" работают на достаточно большом участке территории Венесуэлы. Работают и в столице Каракасе, и на острове Маргарита, куда ездят наши туристы», — заявил дипломат во время конференции «Россия — Латинская Америка». Щетинин добавил, что сейчас ведется работа по внедрению карты на всей территории страны. Оплату картами «Мир» в Венесуэле анонсировал посол России в Каракасе Сергей Мелик-Багдасаров 13 июля. Он заявил, что одним из первых мест, где это будет возможно, станет остров Маргарита. Локации отдали приоритет из-за ее привлекательности для российских туристов. Ранее стало известно, что Россия предложит отменить визы со всеми странами Латинской Америки. Щетинин заявил, что продолжается обсуждение с Гаити, Барбадосом, Багамами, Мексикой, Тринидадом и Тобаго, а также Сент-Люсией.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21702</th>\n",
       "      <td>21702</td>\n",
       "      <td>СУ СК РФ по Карачаево-Черкесской Республике Варвара Кошечкина Группа туристов из Москвы, один из которых был убит жителем Карачаево-Черкесии, ехала на автомобиле Ford Transit в сторону поселка Архыз и остановилась недалеко от станицы Кардоникская в Зеленчукском районе республики. О подробностях произошедшего сообщается на сайте Следственного комитета (СК). «В это время к ним подъехал автомобиль с двумя местными жителями, с которыми у них произошел словесный конфликт, переросший в драку», — говорится в сообщении. Отмечается, что один из местных нанес туристу не менее одного удара ножом в живот, пострадавший умер по пути в больницу. По данным следователей, лицо, совершившее преступление, установлено. Подозреваемый объявлен в федеральный розыск. Как сообщалось ранее, в убийстве обвиняется 33-летний местный житель Абдуллах Бостанов. Известно также имя погибшего — это житель Москвы Олег Албегов. Отмечалось, что турист остановился на обочине, чтобы сходить в туалет, однако это вызвало недовольство у проезжавших мимо местных жителей.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21972</th>\n",
       "      <td>21972</td>\n",
       "      <td>Екатерина Власова В самолете умер 59-летний пассажир, пытавшийся провезти 777 граммов кокаина в животе из Венесуэлы в Россию. Об этом сообщает Telegram-канал SHOT. По его данным, фигурант следовал по маршруту из Порламара в Москву. Во время длительного перелета ему стало плохо, и он умер. Бортпроводники не поняли, что произошло с мужчиной, по прибытии в аэропорт Шереметьево тело забрали врачи. Выяснилось, что в животе россиянина находились 82 контейнера с жидким веществом, содержащим кокаин. Всего было обнаружено более 777 граммов наркотика. Смерть россиянина наступила из-за передозировки. В настоящий момент правоохранители проводят проверку по факту случившегося. Также силовики ищут сообщников погибшего. Ранее стало известно, что в аэропорту Пулково задержали прилетевшую из Стамбула россиянку с кокаиновыми конфетами.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25335</th>\n",
       "      <td>25335</td>\n",
       "      <td>Марина Совина Столичный аэропорт Казахстана в Нур-Султане почти полностью возобновил международное авиасообщение. Об этом сообщает РИА Новости со ссылкой на пресс-службу комитета гражданской авиации министерства индустрии и инфраструктурного развития республики. «Одиннадцатого января года из / в город Нур-Султан планируется выполнение 27 международных рейсов в / из городов Москва, Минск, Дубай, Варшава, Франкфурт, Ташкент, Пхукет, Новосибирск, Коломбо, Анталия, Кутаиси. Отмены рейсов не планируются», говорится в сообщении. В частности, из Актау, Шымкента, Туркестана ожидается выполнение восьми международных рейсов в Гоа, Стамбул, Махачкалу, Грозный. Сообщается также об отмене 20 рейсов из Алма-Аты и Актобе в Стамбул, Дубай, Мале, Ташкент, Москву, Самарканд, Коломбо, Шарджу, Баку, Нукус. Уточняется, что международые рейсы в аэропорту Алматы не будут обслуживаться до 15 января. По данным пресс-службы комитета, всего по стране будут выполнены 35 рейсов, 20 полетов будут отменены. Уточняется, что международное авиасообщение восстановлено полностью, кроме рейсов авиакомпаний Lufthansa и Tukrish Airlines. Ранее сообщалось, что в возобновившем работу аэропорту Нур-Султана усилена охрана. На въезде установлен военный блокпост, однако въезжающие автомобили не досматривают. При входе в аэропорт людей просят предъявить билеты или служебные удостоверения. Протесты в Казахстане начались в воскресенье, 2 января, из-за роста цен на газ для машин с 60 до 120 тенге (с 10 до 20 рублей) за литр. Вскоре экономические требования сменились политическими: отставка правительства, проведение новых выборов, отстранения от власти людей из «клана» бывшего президента Нурсултана Назарбаева.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26174</th>\n",
       "      <td>26174</td>\n",
       "      <td>-канал SHOT Софья Ермакова В Грузии часть скалы обвалилась на территорию средневекового монастыря Шиомгвиме в 30 километрах от Тбилиси. Видео с места происшествия опубликовали в Telegram-канале Shot. Сошедший оползень почти полностью накрыл территорию монастыря. По предварительным данным, пострадавших нет — находившиеся поблизости люди вовремя успели отойти на безопасное расстояние. Шиомгвимский монастырь — средневековый монашеский архитектурный комплекс, первые постройки которого относятся к VI веку. Он расположен в узком известняковом ущелье на северном берегу реки Мтквари. В начале августа масштабный оползень сошел в курортном грузинском городе Шови, почти полностью его разрушив: повреждения затронули всю туристическую инфраструктуру курорта, мосты и дороги, а также коттеджный поселок, где проживали отдыхающие.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1483 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    content  topic\n",
       "40        40                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Дарья Рыбак Россиянка отдохнула в «райском уголке» Краснодарского края под Геленджиком и осталась разочарована поездкой. Отзывом о своем путешествии она поделилась в своем блоге на платформе «Яндекс.Дзен». Туристка уточнила, что всегда считала поселок Голубая бухта, который находится в 10 километрах от Геленджика, «райским уголком». По ее воспоминаниям, место привлекало покоем и тишиной, а также славилось голубой водой и дельфинами, «которые подплывали к пирсу». Тем не менее, последняя поездка в это место разочаровала п его, женщина заметила, что «море не такое голубое как раньше» — цвет воды показался ей «мутным и серым». Из плюсов автор назвала недорогую стоимость жилья — так, большой номер в 15 минутах ходьбы от пляжа обходился ей в 1,2 тысячи рублей в сутки. Ранее в июле эта же путешественница бюджетно отдохнула в поселке Ольгинка в Краснодарском крае и также была разочарована чистотой и инфраструктурой пляжа. По словам автора, приехать на курорт ей посоветовали знакомые, которые были в этом месте несколько лет назад, однако место «полностью разочаровало» россиянку.      7\n",
       "75        75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Мария Гейн Россиянка застряла в Таиланде почти на месяц и рассказала о попытках вернуться домой. Своим опытом она поделилась с порталом «Тонкости туризма». По словам девушки, как только начала поступать информация о массовой отмене рейсов, она решила переждать ограничения. «Первые несколько дней была растерянность, новости были одна страшнее другой», — призналась она. Также путешествен автор успела на один из последних рейсов Air Astana и добралась до России с пересадкой в Казахстане. Ранее в марте сообщалось, что жители Таиланда начали помогать застрявшим в стране российским туристам. Так, они организовали центры помощи и горячие линии для семи тысяч путешественников из России. Отдыхающим, у которых закончился период безвизового пребывания, продлили возможность нахождения в королевстве на 30 суток.      7\n",
       "119      119                                                                                                                                                                                                                                                                                                                                                      Россия и Турция договорились увеличить число авиаперевозок в летнем сезоне. Российские компании смогут выполнять более 720 рейсов по 62 маршрутам, турецкие — 629 рейсов по 37 маршрутам, сообщает «Интерфакс» со ссылкой на Минтранс.Российские авиационные власти уже подтвердили заявки турецких авиакомпаний на дополнительные полеты с суммарной частотой 49 рейсов в неделю. Так, число еженедельно выполняемых рейсов между странами превысит 1,3 тысячи, а это в 1,5 раза больше по сравнению с предыдущим летним сезоном. Согласно договоренностям, еженедельно по маршруту Анталья — Саратов будет совершаться семь рейсов, по маршруту Стамбул — Москва — Анталья — Москва — по 21 ре а кататься по морю на доске, но внезапно усилился ветер, который стал уносить людей от берега. Выбраться отдыхающим помогли другие туристы, которые катались в том же месте на катамаране, — они подтолкнули доску к побережью, где волны были более спокойными. «Женщина с ребенком поплыла, а ветер сильный. (...) И их стало уносить в море, она испугалась, стала орать. Мы, конечно же, пришли на помощь», — рассказала автор видео, отметив, что операция прошла благополучно. Ранее в сети появилось видео, на котором отдыхающие пытались «всем пляжем» спасти женщину, которой стало плохо в море у берега в Сочи. Туристка скончалась на побережье российского курорта до прибытия врачей.      7\n",
       "120      120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Лука Сафронов - Затравкин Фото: @lukasafronovofficial Российский пианист, сын художника Никаса Сафронова, Лука Затравкин раскрыл правду о появившейся в СМИ информации по поводу его заключения в турецком отеле из-за поломанного бассейна. Его слова приводит «Пятый канал». Как рассказал Затравкин, сотрудники гостиницы действительно попросили его компенсировать ремонт бассейна, дно которого было пробито в результате неудачного прыжка музыканта. Тем не менее, по его заверениям, никто не удерживает его в п за ремонт сломанного им бассейна. Отец пианиста сообщил, что музыкант не может вернуться в Россию из-за того, что сотрудники гостиницы заставляют его компенсировать ремонт конструкции — турки оценили работы в 1,4 миллиона рублей. 7 сентября Затравкин получил травму во время отпуска в Турции, прыгнув в детский бассейн. По словам артиста, когда он отдыхал в отеле города Белек и хотел прыгнуть в воду, то перепутал детский бассейн со взрослым. В результате он сильно ударился о дно коленями и поломал дно.      7\n",
       "136      136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Мария Гейн Россия снова стала лидером по турпотоку в Турцию — за первые девять месяцев 2021 года страну посетили 3,46 миллиона россиян. Об этом сообщается на сайте Ассоциации туроператоров России (АТОР). По данным министерства культуры и туризма республики, с января по сентябрь турпоток в страну из-за рубежа составил 17,5 миллиона человек. Это на 86 процентов больше, чем за аналогичный рибыло 1,7 миллиона человек. Замыкают пятерку лидеров Болгария и Иран. Ранее в сентябре сообщалось, что с января по сентябрь 2021 года Сейшельские острова посетили 19,6 тысячи россиян — это на 133 процента больше, чем за аналогичный период в 2019-м. По данным Национального бюро статистики республики, отечественные туристы обогнали иностранных путешественников по объему турпотока в страну, заняв первое место на въездном рынке.      7\n",
       "...      ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...    ...\n",
       "19762  19762                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Александра Качан Карты российской платежной системы «Мир» начали работать на значительной части Венесуэлы. Об этом сообщил директор латиноамериканского департамента Министерства иностранных дел (МИД) России Александр Щетинин, передает ТАСС. «Карты \"Мир\" работают на достаточно большом участке территории Венесуэлы. Работают и в столице Каракасе, и на острове Маргарита, куда ездят наши туристы», — заявил дипломат во время конференции «Россия — Латинская Америка». Щетинин добавил, что сейчас ведется работа по внедрению карты на всей территории страны. Оплату картами «Мир» в Венесуэле анонсировал посол России в Каракасе Сергей Мелик-Багдасаров 13 июля. Он заявил, что одним из первых мест, где это будет возможно, станет остров Маргарита. Локации отдали приоритет из-за ее привлекательности для российских туристов. Ранее стало известно, что Россия предложит отменить визы со всеми странами Латинской Америки. Щетинин заявил, что продолжается обсуждение с Гаити, Барбадосом, Багамами, Мексикой, Тринидадом и Тобаго, а также Сент-Люсией.      7\n",
       "21702  21702                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         СУ СК РФ по Карачаево-Черкесской Республике Варвара Кошечкина Группа туристов из Москвы, один из которых был убит жителем Карачаево-Черкесии, ехала на автомобиле Ford Transit в сторону поселка Архыз и остановилась недалеко от станицы Кардоникская в Зеленчукском районе республики. О подробностях произошедшего сообщается на сайте Следственного комитета (СК). «В это время к ним подъехал автомобиль с двумя местными жителями, с которыми у них произошел словесный конфликт, переросший в драку», — говорится в сообщении. Отмечается, что один из местных нанес туристу не менее одного удара ножом в живот, пострадавший умер по пути в больницу. По данным следователей, лицо, совершившее преступление, установлено. Подозреваемый объявлен в федеральный розыск. Как сообщалось ранее, в убийстве обвиняется 33-летний местный житель Абдуллах Бостанов. Известно также имя погибшего — это житель Москвы Олег Албегов. Отмечалось, что турист остановился на обочине, чтобы сходить в туалет, однако это вызвало недовольство у проезжавших мимо местных жителей.      7\n",
       "21972  21972                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Екатерина Власова В самолете умер 59-летний пассажир, пытавшийся провезти 777 граммов кокаина в животе из Венесуэлы в Россию. Об этом сообщает Telegram-канал SHOT. По его данным, фигурант следовал по маршруту из Порламара в Москву. Во время длительного перелета ему стало плохо, и он умер. Бортпроводники не поняли, что произошло с мужчиной, по прибытии в аэропорт Шереметьево тело забрали врачи. Выяснилось, что в животе россиянина находились 82 контейнера с жидким веществом, содержащим кокаин. Всего было обнаружено более 777 граммов наркотика. Смерть россиянина наступила из-за передозировки. В настоящий момент правоохранители проводят проверку по факту случившегося. Также силовики ищут сообщников погибшего. Ранее стало известно, что в аэропорту Пулково задержали прилетевшую из Стамбула россиянку с кокаиновыми конфетами.      7\n",
       "25335  25335  Марина Совина Столичный аэропорт Казахстана в Нур-Султане почти полностью возобновил международное авиасообщение. Об этом сообщает РИА Новости со ссылкой на пресс-службу комитета гражданской авиации министерства индустрии и инфраструктурного развития республики. «Одиннадцатого января года из / в город Нур-Султан планируется выполнение 27 международных рейсов в / из городов Москва, Минск, Дубай, Варшава, Франкфурт, Ташкент, Пхукет, Новосибирск, Коломбо, Анталия, Кутаиси. Отмены рейсов не планируются», говорится в сообщении. В частности, из Актау, Шымкента, Туркестана ожидается выполнение восьми международных рейсов в Гоа, Стамбул, Махачкалу, Грозный. Сообщается также об отмене 20 рейсов из Алма-Аты и Актобе в Стамбул, Дубай, Мале, Ташкент, Москву, Самарканд, Коломбо, Шарджу, Баку, Нукус. Уточняется, что международые рейсы в аэропорту Алматы не будут обслуживаться до 15 января. По данным пресс-службы комитета, всего по стране будут выполнены 35 рейсов, 20 полетов будут отменены. Уточняется, что международное авиасообщение восстановлено полностью, кроме рейсов авиакомпаний Lufthansa и Tukrish Airlines. Ранее сообщалось, что в возобновившем работу аэропорту Нур-Султана усилена охрана. На въезде установлен военный блокпост, однако въезжающие автомобили не досматривают. При входе в аэропорт людей просят предъявить билеты или служебные удостоверения. Протесты в Казахстане начались в воскресенье, 2 января, из-за роста цен на газ для машин с 60 до 120 тенге (с 10 до 20 рублей) за литр. Вскоре экономические требования сменились политическими: отставка правительства, проведение новых выборов, отстранения от власти людей из «клана» бывшего президента Нурсултана Назарбаева.      7\n",
       "26174  26174                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  -канал SHOT Софья Ермакова В Грузии часть скалы обвалилась на территорию средневекового монастыря Шиомгвиме в 30 километрах от Тбилиси. Видео с места происшествия опубликовали в Telegram-канале Shot. Сошедший оползень почти полностью накрыл территорию монастыря. По предварительным данным, пострадавших нет — находившиеся поблизости люди вовремя успели отойти на безопасное расстояние. Шиомгвимский монастырь — средневековый монашеский архитектурный комплекс, первые постройки которого относятся к VI веку. Он расположен в узком известняковом ущелье на северном берегу реки Мтквари. В начале августа масштабный оползень сошел в курортном грузинском городе Шови, почти полностью его разрушив: повреждения затронули всю туристическую инфраструктуру курорта, мосты и дороги, а также коттеджный поселок, где проживали отдыхающие.      7\n",
       "\n",
       "[1483 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"max_colwidth\", None)\n",
    "sub[sub[\"topic\"] == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_ = 8\n",
    "# sub = df.reset_index()\n",
    "# sub = sub.drop(\"content\", axis=1)\n",
    "# sub[\"topic\"] = class_\n",
    "# sub[[\"topic\", \"index\"]].to_csv(f\"{class_}.csv\", index=False)\n",
    "\n",
    "length_full = 26275\n",
    "classes_distribution = {\n",
    "    0: 0.43393,\n",
    "    1: 0.11945,\n",
    "    2: 0.07057,\n",
    "    3: 0.12451,\n",
    "    4: 0.0965,\n",
    "    5: 0.0137,\n",
    "    6: 0.02778,\n",
    "    7: 0.05116,\n",
    "    8: 0.06236,\n",
    "}\n",
    "distributions = pd.DataFrame([classes_distribution]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"3-epoch-clean-topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26270</th>\n",
       "      <td>3</td>\n",
       "      <td>26270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26271</th>\n",
       "      <td>3</td>\n",
       "      <td>26271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26272</th>\n",
       "      <td>3</td>\n",
       "      <td>26272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26273</th>\n",
       "      <td>3</td>\n",
       "      <td>26273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26274</th>\n",
       "      <td>3</td>\n",
       "      <td>26274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic  index\n",
       "0          0      0\n",
       "1          6      1\n",
       "2          4      2\n",
       "3          0      3\n",
       "4          4      4\n",
       "...      ...    ...\n",
       "26270      3  26270\n",
       "26271      3  26271\n",
       "26272      3  26272\n",
       "26273      3  26273\n",
       "26274      3  26274\n",
       "\n",
       "[26275 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11032</td>\n",
       "      <td>11401</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3430</td>\n",
       "      <td>3138</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1791</td>\n",
       "      <td>1854</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3062</td>\n",
       "      <td>3271</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2619</td>\n",
       "      <td>2535</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>372</td>\n",
       "      <td>359</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>747</td>\n",
       "      <td>729</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1490</td>\n",
       "      <td>1344</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1732</td>\n",
       "      <td>1638</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred   true  diff\n",
       "0  11032  11401   369\n",
       "1   3430   3138   292\n",
       "2   1791   1854    63\n",
       "3   3062   3271   209\n",
       "4   2619   2535    84\n",
       "5    372    359    13\n",
       "6    747    729    18\n",
       "7   1490   1344   146\n",
       "8   1732   1638    94"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.concat(\n",
    "    [sub[\"topic\"].value_counts(), (distributions[0] * length_full).astype(int)], axis=1\n",
    ")\n",
    "comparison.columns = [\"pred\", \"true\"]\n",
    "comparison[\"diff\"] = (comparison[\"pred\"] - comparison[\"true\"]).abs()\n",
    "comparison.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max acc: 0.9509800190294957\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max acc: {1- comparison['diff'].sum() / length_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10120</td>\n",
       "      <td>11401</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3891</td>\n",
       "      <td>3138</td>\n",
       "      <td>753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2064</td>\n",
       "      <td>1854</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3414</td>\n",
       "      <td>3271</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2609</td>\n",
       "      <td>2535</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>395</td>\n",
       "      <td>359</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>501</td>\n",
       "      <td>729</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1483</td>\n",
       "      <td>1344</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1798</td>\n",
       "      <td>1638</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred   true  diff\n",
       "0  10120  11401  1281\n",
       "1   3891   3138   753\n",
       "2   2064   1854   210\n",
       "3   3414   3271   143\n",
       "4   2609   2535    74\n",
       "5    395    359    36\n",
       "6    501    729   228\n",
       "7   1483   1344   139\n",
       "8   1798   1638   160"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.concat(\n",
    "    [sub[\"topic\"].value_counts(), (distributions[0] * length_full).astype(int)], axis=1\n",
    ")\n",
    "comparison.columns = [\"pred\", \"true\"]\n",
    "comparison[\"diff\"] = (comparison[\"pred\"] - comparison[\"true\"]).abs()\n",
    "comparison.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max acc: 0.8849096098953377\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max acc: {1- comparison['diff'].sum() / length_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def splitter(text):\n",
    "#     text = text.split()\n",
    "#     if len(text) > 80:\n",
    "#         text = text[40:]\n",
    "#     return \" \".join(text)\n",
    "# df[\"content1\"] = df[\"content\"].apply(splitter)\n",
    "# df[\"content2\"] = df[\"content1\"].apply(splitter)\n",
    "# df[\"content3\"] = df[\"content2\"].apply(splitter)\n",
    "# df[\"content4\"] = df[\"content3\"].apply(splitter)\n",
    "# df[\"content5\"] = df[\"content4\"].apply(splitter)\n",
    "\n",
    "# from src.utils.utils import get_label2id_id2label\n",
    "\n",
    "# topics = [\n",
    "#     \"Россия\",\n",
    "#     \"Экономика\",\n",
    "#     \"Силовые структуры\",\n",
    "#     \"Бывший СССР\",\n",
    "#     \"Спорт\",\n",
    "#     \"Забота о себе\",\n",
    "#     \"Строительство\",\n",
    "#     \"Путешествия\",\n",
    "#     \"Наука и техника\",\n",
    "# ]\n",
    "# a, _ = get_label2id_id2label(topics)\n",
    "\n",
    "# final = {}\n",
    "# for column in [\"content\", \"content1\", \"content2\", \"content3\", \"content4\", \"content5\"]:\n",
    "#     print(column)\n",
    "#     ans = model(\n",
    "#         df[column].values.tolist(), batch_size=64, truncation=True, max_length=256\n",
    "#     )\n",
    "#     out = []\n",
    "#     for item in ans:\n",
    "#         out.append(a[item[\"label\"]])\n",
    "#     final[column] = out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
